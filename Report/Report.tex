\documentclass[twoside,        %% zweiseitiges Layout
			   11pt,			%% Zeilenabstand
               BCOR10mm,       %% Bindekorrektur 10 mm
%               liststotoc,nomtotoc,bibtotoc, %% Aufnahme der div. Verzeichnisse
                                              %% ins Inhaltsverzeichnis
              % english,ngerman, %% Alternativspr. Englisch, Dokumentspr. Deutsch
               ngerman,english  %% Alternativspr. Deutsch, Dokumentspr. Englisch
%               final,          %% Endversion; draft fuer schnelles Kompilieren
%				smallheadings,
               ]{scrartcl}

\input{header}



\begin{document}

\input{titlepage}

\section{Introduction}
Applying numerical methods in quantum mechanics has always been necessary in analyzing complex structures of quantum mechanical systems. The technical progress of computer performance has enabled physicists and mathematicians to simulate complex many-body systems. With these methods tangible progress in quantum physics can be made to analyze quantum phenomena on the level of many-particle interactions. 

This article tackles the implementation of the Hartree-Fock method for many-body simulations in the functional programming language Haskell. Functional programming languages are getting more and more interesting for physicists through their mathematical way of implementation. In this article simple quantum systems are simulated first to show how a simple one-body system can be simulated straight forward in Haskell.

In chapter \ref{CH:HF}, the main work in this article will be presented, namely the attempt to provide a Haskell function library for simulating physical many-body systems. A general overview of how the implementation can be done as well as problems will be treated in this chapter. Here it is important to note that the full implementation still needs to be completed. Thus, within the progress of this work beyond this report, other problems may arise and might be done in a different manner to increase performance, structure, or simplicity. The implementation so far will be presented as well as the problem of calculating necessary Gaussian integrals.

Although a lot of effort still needs to be put in this project, I hope to provide a structural attempt to show that the implementation of the Hartree-Fock Method in Haskell stands out in its simplistic way, with the strong advantage of reproducibility -- due to its functional operations.

The progress of this project can be tracked on the following repositories:
\begin{itemize}
\item GWDG: \url{https://gitlab.gwdg.de/scientific_practical/project_qm} (University Access only)

\item GitHub: \url{https://github.com/davidschlegel/hartree-fock}
\end{itemize}
A detailed html documentation about important functions for the Hartree-Fock Method can be found in the \url{/dist/doc/html/hartree-fock/} directory.



\section{Simple Quantum Systems}\label{1}
In this chapter simple quantum systems will be studied. Here, we consider a particle in a three-dimensional potential $V(\vec{x})$. The corresponding wave function $\psi(\vec{x}, t)$ is the solution of the Schrödinger equation
\begin{equation}
  i\hbar \frac{\partial}{\partial t} \psi(\vec{x}, t) = H \psi(\vec{x}, t) = - \frac{\hbar^2}{2m}	\Delta \psi(\vec{x}, t)+  V(\vec{x}) \psi(\vec{x}, t) \text{,}
\end{equation}
where $\Delta$ is the Laplacian differential operator: $\Delta = \frac{\partial^2}{\partial x^2} +\frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2} $. For a time-independent potential $V(\vec{x})$, the Schrödinger equation can be formally solved by 
\begin{equation}
     \psi(\vec{x}, t) = \text{U}(t, t_0) \psi(\vec{x}, t_0)   = \exp \left\{ - \frac{i(t-t_0) }{\hbar} H \right\}  \psi(\vec{x}, t_0) \text{.}
\end{equation} 
For a time-dependent potential, like an oscillating laserfield, the time evolution of the wave function becomes
\begin{align}
   \psi(\vec{x}, t) &= \text{U}(t, t_0) \psi(\vec{x}, t_0)  = \hat{\text{T}}_t  \exp \left\{ \frac{i}{h} \int_{t_0}^t H(\tau) \d \tau \right \}  \psi(\vec{x}, t_0)  \\
   &=  \sum_{n = 0}^\infty \frac{1}{n} {\left( \frac{- i}{\hbar} \right) }^n \int_{t_0}^t \d t_1\int_{t_0}^t \d t_2 \dots \int_{t_0}^t \d t_n \hat{\text{T}}_t \left\{ H(t_1)H(t_2) \dots H(t_n) \right\} \text{,} \nonumber
\end{align} 
where $\hat{\text{T}}_t$ is the time ordering operator. A simple approach is to divide the interval $[0 \dots t]$ into a sequence of $N$ steps so that
\begin{equation}
   \text{U}(t, t_0) = \text{U}(t, t_{N-1}) \dots \text{U}(t_2, t_1)\text{U}(t_1, t_0)
\end{equation}   and to neglect small deviations of the Hamiltonian in the small interval $\Delta t = t_n - t_{n-1}$.
   
\subsection{Discretization of the kinetic Energy}
Dividing the Hamiltonian $H$ into $H = T + V$, the non-local kinetic energy operator can be written as
\begin{equation}
    T \psi(\vec{x}, t) = - \frac{\hbar^2}{2m} \Delta \psi(\vec{x}, t) \text{.}
\end{equation}
    
\subsubsection{Method of Finite Differences}
There are different methods and approaches to solve the above equation with numerically. Here we will focus on the approach using finite differences, due to its easy way of implementation, with still very good results. This approach is also computationally stable for many given potential, thus this method can be used in many applications in quantum mechanics./dist/doc/html/hartree-fock/

Taking a grid $(k,l,m)$ in three dimensions, the kinetic energy operator can be approximated by finite differences
\begin{flalign}
   &  T \psi(\vec{x}, t) \approx  - \frac{\hbar^2}{2m}  \left(  
     \frac{\psi_{(k+1,l,m)}^n -2 \psi_{(k,l,m)}^n + \psi_{(k-1,l,m)}^n}{\Delta x^2} +\right. \nonumber \\
     & \left . \frac{\psi_{(k,l+1,m)}^n -2 \psi_{(k,l,m)}^n + \psi_{(k,l-1,m)}^n}{\Delta y^2} +
       \frac{\psi_{(k,l,m+1)}^n -2 \psi_{(k,l,m)}^n + \psi_{(k,l,m-1)}^n}{\Delta z^2} \right),
\end{flalign} with higher order terms $\mathcal{O}(\Delta x^2, \Delta y^2, \Delta z^2 )$ where $n$ represents the discrete time index of the wave function. Considering the time independent Schrödinger equation, we can write the operator in one dimension as a matrix satisfying the eigenvalue equation
 \begin{align}
\left[ \begin{pmatrix}
2	& -1	& &  \dots	 & 0      \\
-1	& 2 	&  -1&  & \vdots	  \\
\vdots	& \ddots 	& \ddots &  \ddots & \\
&  &-1 & 2 & -1\\
0 	&   \dots & & -1	 & 2
\end{pmatrix} + V_{kk} \right ]
\begin{pmatrix}\psi_1 \\ \vdots \\ \psi_k \\ \vdots \\ \psi_N \end{pmatrix}
 = E \begin{pmatrix}\psi_1 \\ \vdots \\ \psi_k \\ \vdots \\ \psi_N \end{pmatrix},
\end{align} where $\vec{\psi}$ are the values of the wave function on the evaluation points $x_k$. $V_{kk}$  represents the potential for each $x_k$, thus it is a diagonal matrix.
\par For the example of a particle in a box the potential is
\begin{align} 
V_{kk} = \left\{ \begin{array}{ll}  \infty  \quad \text{for} \quad  k = 0, \; k= N \\  0 \quad \text{else} \end{array} \right. \text.
\end{align}
Solving the above eigenvalue equation yields the eigenfunctions $\psi_n$ with eigenenergies $E_n$. The boundary conditions $V = \infty$ at $k=0$ and $k=N$ are satisfied even when the boundary conditions are left out. An example of the first four eigenstates is shown in \mbox{ figure \ref{fig:eigenfunctions}.}
\begin{figure}[!htb]
	\Huge
	\centering
	\resizebox{!}{0.4\textwidth}{\input{figures/eigenfunctions.tex}}
	\caption{Propability densities for the calculated eigenfunctions $\psi_n$, with the method of finite differences and a grid of $N=100$ for $n = 1,2,3$ and $4$ in one dimension. For simplicity the pre-factor $\frac{\hbar^2}{2m}$ was set to one, obtaining non-normalized eigenfunctions.}
	\label{fig:eigenfunctions}
\end{figure} 
To reduce the problem of a particle in three dimensions to a simple matrix equation, the spatial wave function $\vec{\psi}$ can be stacked, to obtain a $N^3$ dimensional vector through nested vectors,
\begin{align}
\vec{\psi}_{k,l,m} = \begin{pmatrix}\vec{\psi}_{1,1,m} \\ \vdots \\ \vec{\psi}_{1,N,m} \\ \vdots \\ \vec{\psi}_{N,1,m} \\ \vdots \\ \vec{\psi}_{N,N,m} \end{pmatrix}.
\end{align}

For two dimensions the approximated kinetic energy operator $T$ thus becomes the tridiagonal block matrix
\begin{align}
T \approx  \begin{pmatrix}
4	& -1	&  &  -1	 &     &     &  \scalebox{3}{$0$}  \\
    & \ddots	&     & 		&   \ddots   & 	& \\
    & -1 & 4 &  &  &-1 \\
-1 &      &          & 4     & -1       & & \ddots \\
    & \ddots &     &&    \ddots   &       &  \\
  \scalebox{3}{$0$}   &      & -1       &&         & -1     & 4 \\ 
\end{pmatrix} \text{.}
\end{align}

In figure \ref{fig:2d_eigenfunction} an example of an eigenstate for a particle in a two dimensional box is shown.

\begin{figure}[!htb]
	\Huge
	\hspace*{-1.5in}
	\centering
	\subfigure[$\psi_{12}$, $N=60$.]
		{\resizebox{!}{0.28\textwidth}{\input{figures/2dn12.tex}}\label{fig:2d_eigenfunction}}
	%\hfill
	\subfigure[$\psi_{34}$, $N=100$.]
		{\resizebox{0.48\textwidth}{!}{\includegraphics{figures/2dn34_surf.pdf}}\label{fig:2d_surf_eigenfunction}}\\
	 %\hfill
	\caption{Propability densities for the calculated eigenfunctions with the method of finite differences and a grid of $N=100$ and $N=60$ in two dimensions. For simplicity the pre-factor $\hbar^2 /2m$ was set to one, obtaining non-normalized eigenfunctions.}
	\label{Aufbauskizze}
\end{figure}

An implementation of this eigenvalue problem can be done in the following way:
\begin{code}
--index function
index :: (Int, Int) -> Int -> Double
index (i,j) n
	| i == j       =  4
	| i == j + n   = -1
	| i == j - n   = -1
	| i == j + 1   = -1
	| i == j - 1   = -1
	| otherwise    =  0

-- Laplace Matrix in 2 dimensions
t :: Matrix Double
t = buildMatrix (n*n) (n*n) (\(i,j) -> index (i,j) n)	

--Compute eigenvalues and corresponding eigenvectors 
e = sort(toList(mapVector realPart (fst(eig(t)))))
psi = toColumns (mapMatrix realPart (snd(eig(t))))
\end{code}


%\begin{figure}[!htb]
%	\Huge
%	\centering
%	\resizebox{!}{0.7\textwidth}{\input{figures/2dn12.tex}}
%	\caption{Propability densities for the calculated eigenfunction $\psi_{12}$, with the method of finite differences and a grid of $N=60$ in two dimensions. For simplicity the prefactor $\frac{\hbar^2}{2m}$ was set to one, obtaining a non-normalized eigenfunction.}
%\label{fig:2d_eigenfunction}
%\end{figure} 

In three dimensions, we obtain a $N^3$-dimensional Matrix, for example for $N=2$:
\begin{align*}
T \approx
\tiny
\begin{pmatrix}
 6 & -1 & -1 &  & -1 &  &  & \scalebox{3}{$0$}  \\\\
   -1 & 6 & -1 & -1 &  & -1 &  & \\\\
     -1 & -1 & 6 & -1 & -1 &  & -1 & \\\\
        & -1 & -1 & 6 & -1 & -1 &  & -1\\\\
        -1 &  & -1 & -1 & 6 & -1 & -1 & \\\\
           & -1 &  & -1 & -1 & 6 & -1 & -1\\\\
             &  & -1 &  & -1 & -1 & 6 & -1\\\\
              \scalebox{3}{$0$}  &  &  & -1 &  & -1 & -1 & 6\\\\
              \end{pmatrix}
\end{align*}

Referring back to the time expansion in chapter \ref{1}, the time evolution of an eigenfunction of the Hamiltonian is trivial, since $\psi_t$ only differs in phase from the initial state $\psi_0$, so ${|\psi_t|}^2$ is time-independent, although linear combinations of eigenfunctions depend on time, which will not be calculated here.\\

If we take the computing time into account, it is obvious that even with a small grid size $N<100$ and parallelized computing the simulation of a particle in a square well potential leads to unexecutable applications. The increase of dimensions with constant gridsize results in a growing computing time higher than $\mathcal O (t_0^n)$. \\

To proceed to coupled quantum mechanical systems it seems to be mandatory to use efficient approximations.

\newpage


\section{The Hartree-Fock Method}\label{CH:HF}
    
We consider a real physical system, where particles are not independent, like in atoms, ions, molecules, etc. Assuming the system consists only of $K$ nuclei and $N$ electrons, the Hamiltonian consists of the terms
\begin{align*}
    H = T_i + T_n+ V_{ii} + V_{in} + V_{nn} \text{ ,}
\end{align*} 
where $T_i $ and $T_n$ are the kinetic energies of the electrons and nuclei, respectively. $V_{ii}$ represents the Coulomb repulsion between electrons, $V_{nn}$ between the nuclei and $V_{in}$  the attraction between electrons and nuclei. So the Hamiltonian reads
\begin{align*}
    H &= \sum_{i=1}^N \frac{p_i^2}{2m} + \sum_{n=1}^K \frac{P_n^2}{2M_n} + \frac{1}{4\pi \epsilon_0}\frac{1}{2}\sum_{i,j\neq 1, i \neq j}^N \frac{e^2}{|\mathbf{r}_i - \mathbf{r}_j|} \\ &-  \frac{1}{4\pi \epsilon_0}\sum_{i=1}^K \sum_{i=1}^N \frac{Z_n e^2}{|\mathbf{r}_j-\mathbf{R}_n|} +  \frac{1}{4\pi \epsilon_0}\frac{1}{2} \sum_{n,n'=1;n\neq n'}^K \frac{Z_n Z_{n'} e^2}{|\mathbf{R}_n - \mathbf{R}_{n'}|} \text{,}
\end{align*} where $m$ is the electron mass and $M_n$ the nuclei mass. The index $i$ refers to the electrons, the index $n$ to the nuclei.
This hamiltonian describing the system looks quite complicated and in fact, computing the dynamics of this system seem to be unsolvable even for a few particles and an efficient super computer. Therefore, approximations must be made, still comprising the important information about the system. One approach is the Born-Oppenheimer approximation. It uses the fact that the nuclei are much heavier than the electrons, so the motions of the nuclei are much slower compared to the electrons, justifying to neglect the coulomb repulsion between the nuclei and the kinetic energy of the nuclei, so the approximated Hamiltonian becomes
\begin{align}\label{eq:H_BO}
H_\text{BO} = \sum_{i=1}^N \frac{p_i^2}{2m}  + \frac{1}{4\pi \epsilon_0}\frac{1}{2}\sum_{i,j = 1, i \neq j}^N \frac{e^2}{|\mathbf{r}_i - \mathbf{r}_j|} -  \frac{1}{4\pi \epsilon_0}\sum_{i=1}^K \sum_{i=1}^N \frac{Z_n e^2}{|\mathbf{r}_j-\mathbf{R}_n|}  \text{.}
\end{align}  
The positions of the nuclei can be varied to find the minimum of the total energy.
\subsection{The Hartree-Fock Method and Equations}
In equation (\ref{eq:H_BO}) the antisymmetry of the fermion wave functions was not taken into account. Fock extended the so called \textit{Hartree equation} by taking antisymmetry into account. This approach is based on introducing antisymmetry of the wavefunction by constructing the so called Slater determinant, which is a determinant from single-particle wave functions:
\begin{align*}
\Psi_\text{AS}(\mathbf{x}_1, \dots, \mathbf{x}_N) = \frac{1}{\sqrt{N!}}
\begin{vmatrix}
\psi_1(\mathbf{x}_1) &\psi_2(\mathbf{x}_1)& \dots &\psi_N (\mathbf{x}_1) \\
\psi_1(\mathbf{x}_2) &\psi_2(\mathbf{x}_2)& \dots &\psi_N (\mathbf{x}_2) \\
\vdots & \vdots & & \vdots \\
\psi_1(\mathbf{x}_N)& \psi_2(\mathbf{x}_N)& \dots& \psi_N (\mathbf{x}_N)
\end{vmatrix}
\end{align*} 
Interchanging spatial coordinates of two particles thus changes the sign of the slater determinant. It is important to note that the slater determinant already imposes a restriction to the full wave function. Further improvements could be made to also take linear combinations of slater determinants into account.

The derivation of the \textit{Hartree} and \textit{Hartree-Fock equations} will not be given here, since the main aim here is the implementation of the method. Therefore see e.g. \cite[p. 56-60]{Thijssen2007} and \cite[chapter 3]{Jensen2013}. The \textit{Fock operator} is in natural units given by
 \begin{align} \label{eq:hf}
 \mathcal{F}\psi_k = &\left[-\frac{1}{2} \nabla^2 
 - \sum_n \frac{Z_n}{|\mathbf{r}- \mathbf{R}_n|}\right]\psi_k(\mathbf{x}) + \nonumber
 \sum_{l=1}^N \int \text{d}x' |\psi_l(\mathbf{x}')|^2 \frac{1}{|\mathbf{r}-\mathbf{r}'|} \psi_k(\mathbf{x}) \\
&- \sum_{l=1}^N \int \text{d}x' \psi^*(\mathbf{x}')\frac{1}{|\mathbf{r}-\mathbf{r}'|} \psi_k(\mathbf{x}')\psi_l(\mathbf{x})\text{,}
\end{align}
 satisfying the \textit{Hartree-Fock equation}
\begin{align*}
\mathcal{F} \psi_k = \epsilon_k \psi_k \text{.}
\end{align*} The fourth term in equation (\ref{eq:hf}) is called the exchange term \cite[p. 55]{Thijssen2007} and is nonlocal since the operator is acting on $\psi_k$, but the value at the position $\mathbf{r}$ is determined by the value assumed by $\psi_k$ at all possible positions $\mathbf{r}'$. The eigenvalues $\epsilon_k$ of the Fock operator are related to the total energy by
 \begin{align*}
 E &= \frac{1}{2} \sum_k [\epsilon_k + \braket{\psi_k|h|\psi_k}] \text{, with}\\
 \braket{\psi_k|h|\psi_k} &= \int \text{d}^3\mathbf{x}\; \psi_k^*(\mathbf{x}) \left[-\frac{1}{2} \nabla^2 
 - \sum_n \frac{Z_n}{|\mathbf{r}- \mathbf{R}_n|}\right] \psi_k(\mathbf{x}) \text{.}
 \end{align*} It is obvious that the Fock operator in equation (\ref{eq:hf}) which acts on $\psi_k$ also depends on $\psi_k$ itself.
    
    
    
    
\subsection{The Roothaan Equation}\label{subseq:roothaan}
For molecules numerical basis sets are not efficient, so we use atom centered basis sets to describe the molecular orbital
\begin{align}\label{eq:orbital}
    \psi_k(\vec{x}) = \sum_{p=1}^M C_{pk} \chi_p (\vec{x})\text{,}
\end{align}
with $k = 1,\dots, M$, where $M$ is the number of basis states.
So the Hartree-Fock equation reads
\begin{align*}
    \mathcal{F} (\vec{x})   \psi_k(\vec{x}) &= \epsilon_k    \psi_k(\vec{x})\text{.} \quad \text{Using Eq. (\ref{eq:orbital}) yields}\\
    \mathcal{F} (\vec{x})  \sum_{p=1}^M C_{pk} \chi_p (\vec{x}) &= \epsilon_k \sum_{p=1}^M C_{pk} \chi_p (\vec{x})\text{.}\\
\end{align*}
We can now multiply from the left by a specific basis function $\chi_\nu$. Integrating over the space yields
\begin{align*}
     \sum_{p=1}^M C_{pk} \int \chi_\nu^*(\vec{x})  \mathcal{F} (\vec{x}) \chi_p(\vec{x})\text{d}^3 \vec{x} & =  \epsilon_k  \sum_{p=1}^M C_{pk} \int \chi_\nu^*(\vec{x}) \chi_p(\vec{x})\text{d}^3 \vec{x}   \text{.}
\end{align*}
This can be written in matrix notation of the Hartree-Fock equations, expressed in the atomic orbital basis:
\begin{align}\label{eq:roothaaneq}
    \sum_{p=1}^M C_{pk} F_{\nu p}  &= \epsilon_k \sum_{p=1}^M C_{pk} S_{\nu p}\nonumber\\
    \mathbf{F} \mathbf{C_k} &= \epsilon_k \mathbf{S} \mathbf{C_k} \; \text{,}
\end{align} which is called the \textit{Roothaan equation}.
Since the Fock operator itself depends on $\mathbf{C_k}$ we have the problem of self consistency, because $\mathcal{F}$ has the solution already inside. The Self Consistent Field Method (SCF) uses the approach to solve the above equation iteratively, until the solution deviates only small from the next iteration.
\par Equation (\ref{eq:roothaaneq}) looks similar to the Hartree-Fock equation, but here $\mathbf{S}$ is the overlap matrix for the given orbital basis $\chi_p (\mathbf{r})$. The $\mathbf{S}$-matrix does not vanish, since the used orbital basis does not need to be normalized. In comparison with eq. (\ref{eq:hf}) we can write the Fock matrix as \cite{Thijssen2007}
\begin{align*}
F_{pq} = h_{pq} + \sum\limits_k \sum\limits_{rs} C_{rk}^* C_{sk} \left( 2 \left<pr|g|qs\right> -  \left<pr|g|sq\right> \right)\text{,}
\end{align*} where $h_{pq}$ is identified with the first term of the Fock operator
\begin{align*}
h_{pq} = \left< p|h|q\right> = \int \d^3 r \chi_p^*(\mathbf{r})\left[ -\frac{1}{2} \nabla^2 - \sum\limits_n \frac{Z_n}{\left| \mathbf{R}_n -\mathbf{r} \right|} \right]\chi_q(\mathbf{r})\text{.}
\end{align*} The terms $\left<pr|g|qs\right>$ and $\left<pr|g|sq\right>$ are the two-electron integrals given by
\begin{align*}
\left<pr|g|qs\right> = \int \d^3 r_1 \d^3 r_2 \chi_p^*(\mathbf{r}_1) \chi_r^*(\mathbf{r}_2) \frac{1}{\left| \mathbf{r}_1 -\mathbf{r}_2 \right|} \chi_q(\mathbf{r}_1) \chi_s(\mathbf{r}_2) \text{.}
\end{align*} Here $p$, $q$, $r$ and $s$ label the basis functions and $k$ labels the orbitals $\psi_k$.
For simplicity and implementation it is convenient to define the density matrix as
\begin{align*}
P_{pq} = 2\sum\limits_k C_{pk}^* C_{qk} \text{,}
\end{align*} where the factor $2$ is conventionally due to the spin \cite{Thijssen2007}.
Now the Fock matrix reads
\begin{align*}
F_{pq} = h_{pq} + \sum\limits_{rs} P_{sr} \left( 2 \left<pr|g|qs\right> -  \left<pr|g|sq\right> \right)\text{.}
\end{align*} In this orbital representation the energy is given by
\begin{align*}
E = \sum\limits_{pq} P_{pq}h_{pq} + \frac{1}{2}  \sum\limits_{pqrs} P_{pq}P_{sr} \left[ \left<pr|g|qs\right> -  \frac{1}{2}\left<pr|g|sq\right> \right] \text{.}
\end{align*}

\subsection{Closed and open shell systems}    
Since also the spin plays important role in atomic structures, further restriction on the spin states can be made, leading to closed and open-shell systems, referred to as Unrestricted Hartree-Fock, and Restricted Hartree-Fock. In a closed shell, the orbitals are occupied by two electrons with opposite spin, whereas in an open shell, the energy levels are partially filled where levels contain only one electron. Since the occupied levels in atoms strictly follow the Aufbau principle, the restriction of a closed-shell might lead to unphysical results in some cases. Even if the number of electrons is even, the electrons do not necessarily occupy closed shells -- for an odd number of electrons, an open shell is always present.
For a closed shell, and thus for even $N$, the slater determinant is
\begin{align*}
\Psi_\textbf{R} = \det \left[\alpha \psi_1, \dots, \alpha\psi_{N/2}, \beta \psi_1, \dots, \beta\psi_{N/2} \right] (\mathbf{x}_1, \dots, \mathbf{x}_N) , 
\end{align*} which is referred to as \textbf{restricted closed shell Hartree-Fock} (RHF).
When allowing the number of spin-up and spin-down states to be different, the slater-determinant becomes:
\begin{align*}
\Psi_\textbf{R} = \det \left[\alpha \psi_1, \dots, \alpha\psi_{N_\alpha}, \beta \psi_1, \dots, \beta\psi_{N_\beta} \right] (\mathbf{x}_1, \dots, \mathbf{x}_N).
\end{align*} This represents the \textbf{restricted open shell Hartree-Fock} (ROHF).
However, in the general case, also the one-particle wave functions have spin-dependence, requiring two distinct basis sets, for spin-down and for spin-up states, respectively, yielding to the \textbf{unrestricted Hartree-Fock} (UHF) method, with slater determinant
\begin{align*}
\Psi_\textbf{R} = \det \left[\alpha \psi_1^\alpha, \dots, \alpha\psi_{N_\alpha}^\alpha, \beta \psi_1^\beta, \dots, \beta\psi_{N_\beta}^\beta \right] (\mathbf{x}_1, \dots, \mathbf{x}_N).
\end{align*}
In this general case, the spin-dependence of the basis set is reflected in the expansion coefficients $\mathbf{C}_k$, since they also depend on the spin, yielding the so-called \emph{Pople-Nesbet} equations:
\begin{align*}
\mathbf{F}^\alpha \mathbf{C_k}^\alpha &= \epsilon_k^\alpha \mathbf{S} \mathbf{C_k}^\alpha \\
\mathbf{F}^\beta \mathbf{C_k}^\beta &= \epsilon_k^\beta \mathbf{S} \mathbf{C_k}^\beta
\end{align*}
Here, each equation for spin-up and spin-down orbitals has to be solved independently. However, we will restrict ourselves to RHF for simplicity.

\subsection{Basis set and basis functions}\label{subsec:Basis}
\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{figures/1228px-Gauss_sto4.png}
\caption{Approximation of a slater determinant for the 1s orbital by STO-1G, STO-3G and STO-6G basis functions. \cite{pict}}
\label{fig:STO_GTO}
\end{figure}

A key for obtaining reasonable and accurate results in Hartree-Fock theory is the correct choice of the orbital set on which the calculation is based. As mentioned in section. \ref{subseq:roothaan}, the one-particle wave functions are expanded in the form:
\begin{align*}
    \psi_k(\vec{r}) = \sum_{p=1}^M C_{pk} \chi_p (\vec{r})\text{,}
\end{align*} which is a linear combination of molecular orbitals (LCAO).
Since analytic forms of the desired atomic orbitals are only known for the hydrogen atom, these orbitals need to be approximated in a suitable manner. Following the analytic solution for the hydrogen orbital, we get a general form of atomic orbital basis functions, whith the nucleus centered at $\mathbf{R_\text{A}}$:
\begin{align*}
\chi_\alpha (\mathbf{r}) = r^m P_l (x,y,z) \text{e}^{-\alpha |\mathbf{r-R_\text{A}}|},
\end{align*}
where $\alpha$ defines the range of the orbital. Here, $P_l$ is a polynomial in $x,y$ and $z$ of degree $l$. This orbital type is also known as a Slater type orbital (STO). However, in Hartree-Fock calculations these orbital types are highly disadvantageous, since integrals containing products of STOs are analytically very difficult and in most cases impossible to compute.

Instead, we can avoid this problem, by replacing the simple exponential by a Gaussian function, yielding:
\begin{align*}
\chi_\alpha(\mathbf{r}) = P_L (x,y,z) \text{e}^{-\alpha (\mathbf{r-R_\text{A}})^2} = x^ly^mz^n \text{e}^{-\alpha (\mathbf{r-R_\text{A}})^2}
\end{align*} These functions are called \emph{primitive basis functions}. In this case, products of Gaussian type orbitals (GTO) centered at different nuclei are still Gaussian functions. Thus, the necessary integrals, such as the kinetic integrals, the overlap integrals, the nuclear repulsion integrals as well as the electron-electron interaction integrals can be calculated analytically.
To accurately approximate a Slater-type orbital one can use a linear combination of Gaussian-type orbitals, which is called a \emph{contracted} Gaussian orbital, with contraction coefficients $a_i$. Denoting a primitive Gaussian with $G(\alpha, A, l, m, n)$, a contraction is simply given by a set of primitive Gaussians:
\begin{align*}
G^\text{Contr.} = \sum\limits_i^n a_i G(\alpha_i, A, l, m, n)
\end{align*} 
Here, the variables $l$, $m$ and $n$ are related to the total angular momentum quantum number $L$ by $L = l+m+n$. In Fig. \ref{fig:STO_GTO}, a comparison of different contractions for an 1s orbital is shown. To represent the cusp of the electron probability at $r=0$ for an STO, one needs to take more than one primitive Gaussian into account. Choosing and finding the right basis set is a rather difficult part and there is a huge data base of different basis sets available (see, e.g. \cite{schuchardt2007basis, feller1996role} and the \href{https://bse.pnl.gov/}{EMSL Basis Set Exchange database}). Here, we use the basis set provided by the EMSL Basis Set Exchange database for our calculations.

For a given basis set, the following integrals need to be computed in the Hartree-Fock method:
\begin{itemize}
\item \textbf{Overlap integral}
\begin{align*}
S_{ij} = \left<\alpha_1, \mathbf{A}, l_1, m_1, n_1 |\alpha_2, \mathbf{B}, l_2, m_2, n_2 \right> = \int \psi_1^*(\mathbf{r})
\psi_2(\mathbf{r})d\mathbf{r}
\end{align*}
\item \textbf{Kinetic energy integral}
\begin{align*}
T_{ij} = \bra{\alpha_1, \mathbf{A}, l_1, m_1, n_1}-\frac{1}{2}\nabla^2\ket{\alpha_2, \mathbf{B}, l_2, m_2, n_2} = \int \psi_1^*(\mathbf{r})\left( -\frac{1}{2}\nabla^2\right)
\psi_2(\mathbf{r})d\mathbf{r}
\end{align*}
\item \textbf{Nuclear Attraction integrals}
\begin{align*}
V_{ij}^C = \bra{\alpha_1, \mathbf{A}, l_1, m_1, n_1}\frac{Z_C}{r_C}\ket{\alpha_2, \mathbf{B}, l_2, m_2, n_2} = \int \psi_1^*(\mathbf{r})\left(\frac{Z_C}{r_C}\nabla^2\right)
\psi_2(\mathbf{r})d\mathbf{r}
\end{align*}
\item \textbf{Electron repulsion integral}
\begin{align*}
\text{ERI} = \left<pr|g|qs\right> = \int \d^3 r_1 \d^3 r_2 \psi_p^*(\mathbf{r}_1) \psi_r^*(\mathbf{r}_2) \frac{1}{\left| \mathbf{r}_1 -\mathbf{r}_2 \right|} \psi_q(\mathbf{r}_1) \psi_s(\mathbf{r}_2)
\end{align*}
\end{itemize} The last integral is by far the most important and also most difficult part since the calculation involving 4 different primitive Gaussians scales with $N^4$, where $N$ is the number of primitive Gaussians. Using very efficient state-of-the-art algorithms, the calculation of the electron repulsion integral can be reduced to $\mathcal{O}(N)$.
For these integrals, analytic solutions exists. The implementation of these integrals is straight-forward but very tedious. For a detailed calculation of Gaussian integral evaluation, see \cite{fermann1997fundamentals, petersson2009detailed, gill1989efficient, huzinaga1985basis, raffenetti1973general}. 
The analytical computations of these functions are very advantageous, since they are represented by pure functions, i.e. no side-effects or the underlying random system or memory affects the functions, making the computations completely reproducible.


    
\subsection{Program Structure}
Here a general overview of the general program structure will be given.

\begin{enumerate}
	\item \textbf{Input data}:\\
	Here the basis sets as well as the geometry of the atoms in the molecule will be provided and parsed to the program. Furthermore the numbers of electrons $N$ and the atomic charge numbers $Z_n$ are needed as input.
	\item \textbf{Calulate independent matrices}\\
	All matrices that are independent of the eigenvectors $\mathbf{C}_k$ can be calculated:
	\begin{itemize}
		\item The overlap matrix $S_pq$,
		\item The two-electron integrals $\braket{pr|g|qs}$,
		\item The uncoupled one-electron Hamiltonian $h_{pq}$.
	\end{itemize}
	\item \textbf{Make an initial guess for the density matrix $\mathbf{P}$.}\\
	A possibility is to use $\mathbf{P} = 0$ as an initial guess, which means that the electrons only feel the nuclei but not each other.
	\item \textbf{Self-consistency procedure}\\
	This program section is the most important, since in solves the Roothaan equation recursively. It consist of the following steps.
	\begin{itemize}
		\item Calculate the Coulomb and exchange contributions to the Fock matrix,
		\item Construct the Fock matrix,
		\item Solve the Roothaan equation by diagonalization of the Fock matrix for the given density matrix $\mathbf{P}$,
		\item With the obtained eigenvectors a new density matrix is $\mathbf{P}$ is constructed.
	\end{itemize}
	\item \textbf{Output data}\\
	The output is are the converged eigenstates $\mathbf{C}_k$ and its corresponding Fock levels.
\end{enumerate}

In figure \ref{skiz:programstructure} a schematic representation of the general program scheme is shown. 
    
\tikzstyle{decision} = [diamond, draw, fill=blue!20, text width=7em, text badly centered, node distance=5.7cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20,  text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw]
\tikzstyle{largeblock} = [rectangle, draw, fill=blue!20, text width=17em, text centered, rounded corners, minimum height=4em]
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm, minimum height=2em]
\tikzstyle{c} = [rectangle, draw, inner sep=0.5cm, dashed]
\begin{figure}[!htb]
\centering
\scalebox{0.8}{   
\begin{tikzpicture}[node distance = 3.3cm, auto]
    % Place nodes
    \node [block] (init) {choose set of basis functions $\chi$};
    \node [block, below of=init] (hpq) {guess density matrix $\mathbf{P}$};
    \node [block, left of=hpq] (Spq) {overlap matrix $S_{pq}$};
    \node [block, right of=hpq] (prgqs) {two-electron integrals $\braket{pr|g|qs}$};
    \node [block, right of=prgqs] (Ppq) {uncoupled one-electron Hamiltonian $h_{pq}$};
    \node [block, below of=Spq] (VSV) {bring $\mathbf{S}$ to unitform, s.th $\mathbf{V^\dagger SV}  = \mathbf{1}$};
    
    \node [largeblock, scale=0.9, below of=prgqs] (G) {Coulomb and exchange contributions to the Fock matrix. {\small $G_{pq}=\sum_{rs}P_{rs} \left[\braket{pr|g|qs} - \frac{1}{2} \braket{pr|g|sq}\right]$}};
    \node [block, below left of=G] (F) {Fock matrix \mbox{$F_{pq} = h_{pq} + G_{pq}$}};
    \node [block, below of=F] (diagF) {solving $\mathbf{F C} = \epsilon \mathbf{C}$ by diagonalization of $\mathbf{F}$};
    \node [block, below of=diagF] (newP) {construct  new density matrix through \mbox{$ P_{pq} = 2 \sum C_{pk} C_{pk}^*$}};
     \node [decision, right of=newP] (convergence) {convergence};
     \node [block, below right of=convergence] (done) {done};
     
     \node [c,fit=(G) (newP) (convergence)] (container) {};
 

    
    \path [line] (init) -| (Spq);
    \path [line] (init) -| (Ppq);

    \path [line] (init) -- (hpq);
    \path [line] (init) -| (prgqs);
    \path [line] (Spq) -- (VSV);
    \path [line] (G) |-	 (F);
    \path [line] (F) -- (diagF);
    \path [line] (diagF) -- (newP);
    \path [line] (VSV) |- (newP);
    \path [line] (prgqs) -- (G);
    \path [line] (Ppq) |- (F);
    \path [line] (hpq) |- (G);
    
     \path [line] (newP) |- (convergence);
    \path [line] (convergence) |- node [near start] {no} (G);
    \path [line] (convergence) -| node [near end] {yes} (done); 
    
\end{tikzpicture}
}

\caption{General scheme of the program structure.}
\label{skiz:programstructure}
\end{figure}
\pagebreak
To solve the Roothaan equation, which is a generalized eigenvalue equation, one can reduce this equation to a normal eigenvalue equation, by finding a matrix $\mathbf{V}$, such that $\mathbf{V}^\dagger\mathbf{S}\mathbf{V} = \mathbf{1}$. In the program, we use build-in functions to solve the entire generalized eigenvalue equation.

\subsection{Implementation}
Here, an overview about the implementation so far will be given. Not all of the implemented functions are mentioned. A detailed program documentation can be found in the \texttt{/dist/doc/html/hartree-fock/} directory. A schematic scheme of the different program modules is shown in Fig. \ref{scheme:modules}.


\tikzstyle{block} = [rectangle, draw,  text width=8em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex]
\tikzstyle{redline} = [draw=red!50, dash pattern=on 3pt off 3pt, -latex]
\tikzstyle{largeblock} = [rectangle, draw, text width=10em, text centered, rounded corners, minimum height=4em]
\tikzstyle{largerblock} = [rectangle, draw, text width=15em, text centered, rounded corners, minimum height=4em]

\tikzstyle{c} = [rectangle, draw, inner sep=0.5cm, dashed]
\tikzstyle{missingblock} = [rectangle, dash pattern=on 3pt off 3pt, fill=red!50,  text width=12em, text centered, rounded corners, minimum height=4em]
\begin{figure}[!htb]
\centering

\scalebox{0.8}{   
\begin{tikzpicture}[node distance = 3.3cm, auto]
    % Place nodes
    \node [largerblock] (init) {\textbf{Module management} \\ \vspace{5pt} \texttt{HF}};
    \node [largeblock, below of=init] (ReadWrite) {\textbf{Programm interaction}\\ \vspace{5pt} \texttt{ReadWrite}};
    \node [largeblock, left =10pt of ReadWrite] (Data) {\textbf{Data type declarations and Data type functions}\\ \vspace{5pt} \texttt{Data}};
    \node [largeblock, right =10pt of ReadWrite] (Gauss) {\textbf{Gaussian integral evaluation}\\ \vspace{5pt} \texttt{Gauss}};
    \node [missingblock, below of=ReadWrite] (SCF) {\textbf{Self-Consistent-Field method}\\ \textbf{Eigenvalue Solver}\\ \vspace{5pt} \texttt{SCF}};

	\path [line] (init.south) -- ++(0,-0.5) -- (ReadWrite);
    \path [line] (init.south) -- ++(0,-0.5) -| (Data);
    \path [line] (init.south) -- ++(0,-0.5) -| (Gauss);
    \path [redline] (init.south) -- ++(0,-0.5) -- ++(-7.5,0) |- ([yshift=-6pt]SCF.west);

    \path [line] (ReadWrite.west) -- (Data.east);
    \path [line] ([xshift=-6pt]Gauss.north) --++(0,0.5)  -| ([xshift=+6pt]Data.north);
	\path [redline] (SCF) -| (Data);
	\path [redline] (SCF) -| (Gauss);
	\path [redline] (SCF.north) -- ++(1,0) -| (ReadWrite.south);
    
     %\path [line] (newP) |- (convergence);
    %\path [line] (convergence) |- node [near start] {no} (G);
    %\path [line] (convergence) -| node [near end] {yes} (done); 
    
\end{tikzpicture}}
\caption{Schematic modular program structure. Arrows represent module dependencies. The \texttt{SCF} module still needs to be implemented.}
\label{scheme:modules}
\end{figure}

\subsubsection{Data type declarations and Data type functions}
Specific Data types are necessary to maintain an overview about the entire program structure and to simplify calculations. Therefore we define the following data types:
\begin{itemize}
\item \texttt{Orbital}, defining a certain orbital; dependencies: None
\item \texttt{Atom}, defining a collection of several (contracted) Orbitals; dependencies: Orbital
\item \texttt{Mol}, defining a Structure containing multiple Atoms; dependencies: \texttt{Atom, Orbital}
\item \texttt{PG}, defining a primitive Gaussian function; dependencies: None
\item \texttt{Ctr}, defining a set of contractions of primitive Gaussians; dependencies: \texttt{PG}
\end{itemize}
For all data types we use a \emph{record syntax}, to access the information. \\ \\

\large{\texttt{Orbital}}\\
The \texttt{Orbital} defines all information about a certain (contracted) Gaussian Orbital. Data type contains the same information as \texttt{CTr}, but it is a canonical data structure for importing data using the basis set database. 
\begin{code}
data Orbital = Orbital { 
			-- | orbital-type (angular momentum) and contraction length  (see also 'momentumdict') 
			description :: (String, Int)
			-- | numbering of gaussians
			, numbering :: Vector Double
			-- | exponents of gaussians  
		 	, exponents :: Vector Double
			-- | coefficients of gaussians  
			, coeffs :: Vector Double  
			} deriving (Eq, Show)
\end{code} \texttt{description} contains the angular momentum information (eg. \texttt{"S", "P", "D"}) and the length of contraction, i.e. the number of contraction coefficients. The exponents and the contraction coefficients are stored as vectors. \\ \\

\large{\texttt{Atom}}\\
To store all orbital information for a certain atom, we use the data type \texttt{Atom}:
\begin{code}
data Atom = Atom {atomname :: String	-- ^ name of the Atom e.g. \"CARBON\"  
		, orbitals :: [Orbital] -- ^ list of the orbitals corresponding to an atom   
		 } deriving (Eq, Show)
\end{code}Here, \texttt{atomname} is a \texttt{String} used to identify the atom (eg. \texttt{"CARBON"}) and its corresponding atomic number $Z$. \\ \\


\large{\texttt{Mol}}\\
To construct an entire molecule which is the input data type for the Hartree-Fock calculations, we provide a data type containing multiple \texttt{Atom} data types, with corresponding position vectors:
\begin{code}
data Mol = Mol {  molname :: String		-- ^ Mol name , eg \"H20\" etc.  
		, config :: [(Atom , Vector Double)] -- ^ configuration  
		} deriving (Eq, Show)
\end{code}
To easily compute the entire geometry of the system, the number of basis functions and the number of atoms, we provide the functions
\begin{code}
geometry :: Mol -> [Vector Double]

natoms :: Mol -> Int

nbasisfunctions :: Mol -> Int
\end{code}
Additionally, we can merge a list of \texttt{Mol} data types into a single \texttt{Mol} data type, which makes it easy to construct larger molecular structures, with repeating properties, s.a. benzenes, phenyls, etc.:
\begin{code}
mergeMols :: [Mol] -- ^ List of Mol datatypes
			-> String -- ^ New strucure name
			-> Mol -- ^ final superstructure Mol
\end{code}\\ \\

\large{\texttt{PG} and \texttt{Ctr}}\\
For matrix calculations including primitive Gaussian functions, it is convenient to use a data type with all necessary information:
\begin{code}
data PG = PG { lmn :: [Int]	-- ^ \[l,m,n\] angular momentum information  
		, alpha :: Double -- ^ exponent  
		, position :: Vector Double -- ^ position vector  
		} deriving (Eq, Show)
\end{code}
For convenience, we define a contraction, \texttt{Ctr}, as a list of primitives, with corresponding contraction coefficients:
\begin{code}
data Ctr = Ctr{ gaussians :: [PG]	-- ^ list of primitives  
		, coefflist :: Vector Double -- ^ coefficient vector  
		} deriving (Eq, Show)
\end{code} \\

We can convert a \texttt{Mol} data type to a list of \texttt{Ctr} data types, which is necessary for matrix computations involving Gaussian functions:
\begin{code}
mol_to_gaussians :: Mol -> [Ctr]
\end{code}

\subsubsection{Input data}
For the input of basis set data, we integrate an open source external \texttt{Python} interface, \href{https://github.com/TApplencourt/EMSL_Basis_Set_Exchange_Local.git}{Emsl basit set exchange local} provided by \href{https://github.com/TApplencourt}{Thomas Applencourt} (for more information, see \url{https://github.com/TApplencourt/EMSL_Basis_Set_Exchange_Local.git}). To generate basis set data for a given atom and orbital type, (eg. \texttt{"STO-6G"}), we save the basis set data to a file, for each atom resoectively by the function
\begin{code}
generateFile:: [Char] -- ^ basis, eg."STO-6G"
		-> [Char] -- ^ elementsymbol, eg. "C"
		-> [Char] -- ^ output filename
		-> IO (Maybe Handle, Maybe Handle, Maybe Handle, ProcessHandle) -- ^ output file is saved to @ EMSL_Basis_Set_Exchange_Local/data/ @
\end{code}
From a given file with basis set information, we can then parse the basis set information from the file into the \texttt{Haskell} program. Thus, one file with generated basis set data gives a single \texttt{Atom} data type. This is done using the function
\begin{code}
getAtomData :: [Char] -- ^ filename of the file where the basis set for a certain element is stored
		-> Atom -- ^ Atom datastructure
\end{code}
Additionally, we can read geometry information for given atoms stored in a file. This is especially useful when comparing results with existing Hartree-Fock programms. Here, we use the xyz-formatting, which is also used by the \href{http://www.cmbi.ru.nl/molden/}{Molden Software}. Since constructing a complex molecular structure with a good guess of initial geometry can be rather difficult from scratch, the molecule with its geometry can be constructed via Molden and then used as input in our program. This can be done in the following way.
\begin{enumerate}
\item Read a file containing geometry information and output a list of element symbols, e.g. \texttt{"C"} with corresponding positions.
NOTE: The file should have the cartesian XYZ-file format that is used in Molden.
\begin{code}
getgeom :: [Char] -- ^ directory
	-> [([Char], Vector Double)] -- ^ List with element symbols and corresponding position vectors
\end{code}
example:
\textbf{\texttt{> > > let list = getgeom "CO2.xyz"}}
\item With \texttt{constr\_set\_from\_file}, construct basis set data for the different elements which will be saved to files
\begin{code}
constr_set_from_file :: [([Char], Vector Double)] -> [Char] -> IO ()
--For a list of element symbols with corresponding positions ([([Char], Vector Double)]) and a given basis set, e.g. "STO-3G", save basis set data to multiple files.
\end{code}
example:\textbf{\texttt{> > > constr\_set\_from\_file list "STO-6G"}}
\item With \texttt{get\_mol\_from\_files}, read the basis set data to construct the complete molecule:
\begin{code}
get_mol_from_files :: [([Char], Vector Double)] -- ^ list of element symbols with corresponding positions
		-> [Char] -- ^ basis set name, eg. \"STO-3G\"
		-> [Char] -- ^ molname, eg. \"CO2\"
		-> Mol
\end{code}
example:\textbf{\texttt{> > > let mol = get\_mol\_from\_files list "STO-6G" "CO2"}}
\end{enumerate}

\subsubsection{Output}
Information about the constructed \texttt{Mol} data type can obtained by the function
\begin{code}
molInfoPrint :: Mol -> String
{-__Output:__ 
	Molecule Name
	Number of Atoms
	For each Atom:
		* Atomname
		* Number of Orbitals
		* contracted set descriptions
		* geometry
-}
\end{code}
example:\\
\textbf{\texttt{> > > let mol = Mol "SINGLE-Carbon" [(getAtomData "C\_STO\_3G.dat", fromList [1,0,0])]
}}\\
\textbf{\texttt{> > > putStrLn \$ molInfoprint mol
}} \\
\textbf{\texttt{Molecule name:   SINGLE-Carbon}}\\
\textbf{\texttt{Number of Atoms:  1}}\\
\quad \textbf{\texttt{Atom:    CARBON}}\\
\quad \textbf{\texttt{Number of Orbitals:	3}}\\
\quad \quad \textbf{\texttt{contracted sets:	[("S",3),("S",3),("P",3)]}}\\
\quad \quad \textbf{\texttt{geometry:	 	[1.0,0.0,0.0]}}
Additionally, only geometry information can be obtained and by
\begin{code}
molGeomPrint :: Mol -> String
\end{code}
\subsubsection{Gaussian integral evaluation}
The Gaussian integral evaluation is so far the most difficult and tedious part. The analytical expressions for the integrals in Sec. \ref{subsec:Basis} are difficult to compute and involves double factorial and error function evaluation. If the angular quantum number $L$ is larger than zero, which is the case for most of the orbitals in atoms, the calculation become somewhat difficult. A general summation of functions used for Gaussian integral evaluation:
\begin{itemize}
\item \textbf{Normalization}\\
For primitive Gaussians as well as for an entire Contraction, the normalization constant needs to be computed:
\begin{code}
normCtr :: Ctr -> Double
--Calculates normalization factor of contracted Gaussian with arbitrary angular momentum
normPG :: PG -> Double
--Calculates normalization factor of primitive Gaussian with arbitrary angular momentum
\end{code}
\item \textbf{Contraction operations}
\begin{itemize}
\item
\begin{code}
-- |Evaluates a given function for two contractions. The operation is symmetric in contraction arguments.
zipContractionWith :: (PG -> PG -> Double) -- ^ Function of two primitive Gaussians 
			-> Ctr -- ^ Contraction
			-> Ctr -- ^ Contraction 
			-> Double
\end{code}
\item \begin{code}
-- |Construct a matrix out of a list of Contractions and a function for two primitive gaussians
constr_matrix :: [Ctr] -- ^ List of Contraction
		-> (PG -> PG -> Double) -- ^ Function of two primitive Gaussians
		-> Matrix Double -- ^ Matrix of dimensions (length [Ctr]) x (length [Ctr])
\end{code}\end{itemize} 
\item \textbf{Gaussian integrals}
\begin{itemize}
\item \texttt{s\_12}: Overlap integral\\
\begin{align*}
S_{ij} = \left<\alpha_1, \mathbf{A}, l_1, m_1, n_1 |\alpha_2, \mathbf{B}, l_2, m_2, n_2 \right>
\end{align*}
\begin{code}
-- |Calculates overlap integral of two primitive gaussians
s_12 :: PG -> PG -> Double
\end{code}
\item \textbf{Kinetic energy integral}\\
\begin{align*}
T_{ij} = \bra{\alpha_1, \mathbf{A}, l_1, m_1, n_1}-\frac{1}{2}\nabla^2\ket{\alpha_2, \mathbf{B}, l_2, m_2, n_2}
\end{align*}
The calculation of the kinetic energy integral is based on the calculation of the overlap integral.
\begin{code}
-- |Calculates kinetic energy integral of two primitive gaussians
t_12 :: PG -> PG -> Double
\end{code}
\item \textbf{Nuclear attraction integral}\\
\begin{align*}
V_{ij}^C = \bra{\alpha_1, \mathbf{A}, l_1, m_1, n_1}\frac{Z_C}{r_C}\ket{\alpha_2, \mathbf{B}, l_2, m_2, n_2}
\end{align*}
\emph{Note: The calculation of the nuclear attraction integral is at the present state not complete.} For the calculation of the nuclear attraction integral, also the atomic number $Z_C$ as well as the distance to the nucleus $r_C$ are needed.
\begin{code}
-- |Calculates nuclear attraction integral of two primitive gaussians
-- | Here, the second argument is the position vector of the nucleus.
v_12 :: Double -> Vector Double -> PG -> PG -> Double
\end{code}
\item \textbf{Electron repulsion integrals}\\
\begin{align*}
\left<pr|g|qs\right> = \int \d^3 r_1 \d^3 r_2 \psi_p^*(\mathbf{r}_1) \psi_r^*(\mathbf{r}_2) \frac{1}{\left| \mathbf{r}_1 -\mathbf{r}_2 \right|} \psi_q(\mathbf{r}_1) \psi_s(\mathbf{r}_2)
\end{align*} \emph{Note: The calculation of the electron repulsion integrals is no yet complete. Different algorithms and recurrence relations can be used to make the calculation simpler and faster. However, the implementation of the electron repulsion integral is one of the most tedious and difficult parts in this program.}
\end{itemize}
\end{itemize}

\section{Outlook}

\pagebreak

    % bib stuff
    \nocite{*}
\bibliography{bibfile} 
\end{document}
